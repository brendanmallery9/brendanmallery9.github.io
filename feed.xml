<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://brendanmallery9.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://brendanmallery9.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-05-04T11:21:06+00:00</updated><id>https://brendanmallery9.github.io/feed.xml</id><title type="html">Brendan Mallery</title><subtitle>&apos;PhD candidate studying optimal transport&apos;
</subtitle><entry><title type="html">Derivation of the Iterative Shrinkage and Thresholding Algorithm (ISTA)</title><link href="https://brendanmallery9.github.io/blog/2025/ISTA/" rel="alternate" type="text/html" title="Derivation of the Iterative Shrinkage and Thresholding Algorithm (ISTA)" /><published>2025-03-10T00:00:00+00:00</published><updated>2025-03-10T00:00:00+00:00</updated><id>https://brendanmallery9.github.io/blog/2025/ISTA</id><content type="html" xml:base="https://brendanmallery9.github.io/blog/2025/ISTA/"><![CDATA[<p>We are interested in solving:</p>

\[\hat{x} = \text{argmin}_{x} \frac{1}{2} \|b - Ax\|_2^2 + \lambda \|x\|_1.\]

<p>Denote the objective function as $f(x) = \frac{1}{2} |b - Ax|_2^2 + \lambda |x|_1$. We will derive an algorithm for solving this.</p>

<p>Let $A = I$. Then the problem has the objective:</p>

\[f(x) = \frac{1}{2} \|b - x\|_2^2 + \lambda \|x\|_1 = \sum_{i=1}^n \left( \frac{1}{2} |b_i - x_i|^2 + \lambda |x_i| \right).\]

<p>This function is a sum of functions of the form:</p>

<p>\(g(\tau) = \frac{1}{2} |\gamma - \tau|^2 + \lambda |\tau|, \; \; \; \; \gamma \in \mathbb{R}.\)
We compute the subdifferential</p>

\[\partial g(\tau) = \tau - \gamma + \lambda \partial|\tau|,\]

<p>where</p>

\[\partial|\tau| = 
\begin{cases} 
-1 &amp; \text{for } \tau &lt; 0, \\
1 &amp; \text{for } \tau &gt; 0, \\
[-1, 1] &amp; \text{for } \tau = 0,
\end{cases}\]

<p>which can be easily computed from the definition. At the unique optimal $\tau^*$ (for each $\lambda, \gamma$), we must have</p>

\[0 \in \partial g(\tau^*).\]

<p>We can now compute an expression for an optimal $\tau^<em>$ as a function of $\lambda, \gamma$. If $\tau^</em> &gt; 0$, we have $0 \in \partial g(\tau^*)$, which implies that</p>

\[\tau^* = \gamma - \lambda.\]

<p>Since $\tau^*&gt;0$, this can only be true when $\gamma \geq \lambda$, and $\gamma \geq 0$, and hence we have determined</p>

\[\tau^* = \gamma - \lambda \quad \text{on} \quad [\lambda, \infty).\]

<p>Similarly, we compute that</p>

\[\tau^* = \gamma + \lambda \quad \text{on} \quad (-\infty, \lambda].\]

<p>Finally, we have the case of $\tau^* = 0$, which implies that</p>

\[0 \in -\gamma + \lambda [-1, 1],\]

<p>and hence $\gamma \in [-\lambda, \lambda]$.</p>

<p>This is a piecewise linear function. Another way of representing this is</p>

\[\tau^* = ST_\lambda(\gamma),\]

<table>
  <tbody>
    <tr>
      <td><strong>Definition 1</strong> The soft-thresholding function is $ST_\lambda(\gamma):=sgn(\gamma)\max(</td>
      <td>\gamma</td>
      <td>-\lambda,0).$</td>
    </tr>
  </tbody>
</table>

<p>Hence we have characterized optimal $\hat{x}$ as having coordinates described by $ST_\lambda(b_i)$.</p>

<p>We now prove the result for general PSD $A$:</p>

\[h(x, x^{(0)}) = \frac{c}{2} \|x - x^{(0)}\|_2^2 - \frac{1}{2} \|Ax - Ax^{(0)}\|_2^2.\]

<p>For this function to be strictly convex in $x$, we must have:</p>

\[c &gt; \|A^\top A\|_2 = \lambda_{\max}(A^\top A).\]

<p>Define the new objective function:</p>

\[\tilde{f}(x, x^{(0)}) = f(x) + h(x, x^{(0)}) = \frac{1}{2} \|b - Ax\|_2^2 + \lambda \|x\|_1 + \frac{c}{2} \|x - x^{(0)}\|_2^2 - \frac{1}{2} \|Ax - Ax^{(0)}\|_2^2.\]

<p>Our goal will be to reduce this to something resembling the case with $A=I$, so we introduce the variable:</p>

\[\nu_0 = x^{(0)} + \frac{1}{c} A^\top (b - Ax^{(0)}).\]

<p>Then, by some algebra, we have:</p>

\[\tilde{f}(x, x^{(0)}) = \frac{1}{2} \|b - Ax\|_2^2 + \lambda \|x\|_1 + \frac{c}{2} \|x - x^{(0)}\|_2^2 - \frac{1}{2} \|Ax - Ax^{(0)}\|_2^2 = C + \lambda \|x\|_1 + \frac{c}{2} \|x - \nu_0\|_2^2\]

<p>where $ C $ doesn’t depend on $ x $ or $ x^{(0)} $. From before, we see that this is minimized by:</p>

\[x^{(0),*} = ST_{\frac{\lambda}{c}}(\nu_0) = ST_{\frac{\lambda}{c}} \left( x^{(0)} + \frac{1}{c} A^\top (b - Ax^{(0)}) \right)\]

<p>Thus,</p>

\[x^{(0),*} = \text{argmin}_x \tilde{f}(x, x^{(0)}) = \text{argmin}_x f(x) + h(x, x^{(0)}).\]

<p>This suggests (and derives) the proximal algorithm for this objective function and regularization:</p>

\[x^{(i+1)} = ST_{\frac{\lambda}{c}} \left( x^{(i)} + \frac{1}{c} A^\top (b - Ax^{(i)}) \right).\]

<p>This is the ISTA algorithm. More generally, we can change the $ \ell^1 $ regularization to get iterates of the form:</p>

\[x^{(i+1)} = T_{\frac{\lambda}{c}} \left( x^{(i)} + \frac{1}{c} A^\top (b - Ax^{(i)}) \right).\]]]></content><author><name>Brendan Mallery</name></author><category term="Convex Optimization" /><category term="Dictionary Learning" /><category term="Regularization" /><summary type="html"><![CDATA[We are interested in solving:]]></summary></entry><entry><title type="html">Characterization of KL-divergence Barycenters</title><link href="https://brendanmallery9.github.io/blog/2025/KL_divergence_barycenters/" rel="alternate" type="text/html" title="Characterization of KL-divergence Barycenters" /><published>2025-01-05T00:00:00+00:00</published><updated>2025-01-05T00:00:00+00:00</updated><id>https://brendanmallery9.github.io/blog/2025/KL_divergence_barycenters</id><content type="html" xml:base="https://brendanmallery9.github.io/blog/2025/KL_divergence_barycenters/"><![CDATA[<p>In this post I’ll record a simple but instructive calculation, giving a basic template for characterizing optimizers of functionals on probability space: Let $\Delta^m := {(\lambda_1, \lambda_2, \dots, \lambda_m) \in \mathbb{R}^m \mid \lambda_j \geq 0,\ \sum_{j=1}^m \lambda_j = 1}$ be the $m$-dimensional probability simplex. Let $\mathcal{V} = {\nu_1, \dots, \nu_m}$ be a family of probability measures on $\Omega \subseteq \mathbb{R}^d$ such that $\nu_j \propto \exp(-V_j)$ for some nonnegative continuous functions $V_j$.</p>

<p>For any $\lambda \in \Delta^m$, define 
$\mathcal{K}<em>{\lambda, \mathcal{V}}(\mu) := \sum</em>{j=1}^m \lambda_j\ \mathrm{KL}(\mu \mid \nu_j)$, 
where $\mathrm{KL}$ denotes the Kullback–Leibler divergence.</p>

<p>Let $\mu_\lambda := \operatorname*{argmin}<em>{\rho \in \mathcal{P}(\Omega)} \mathcal{K}</em>{\lambda, \mathcal{V}}(\rho)$. We wish to characterize the set 
$M := {\mu_\lambda \mid \lambda \in \Delta^m}$.</p>

<p>We will do this using first-order calculus on the space of probability measures.</p>

<p><strong>Definition 1</strong><br />
Let $\mathcal{P}<em>{\text{abs}}(\Omega) \subseteq \mathcal{P}(\Omega)$ be the set of probability measures which are absolutely continuous with respect to the Lebesgue measure, and let $\mathcal{F} : \mathcal{P}(\Omega) \to \mathbb{R} \cup {+\infty}$ be a functional.<br />
If $\mathcal{F}(\rho) &lt; \infty$ for all $\rho \in \mathcal{P}</em>{\text{abs}}(\Omega)$, we say that $\mathcal{F}$ is <strong>regular</strong>.</p>

<p><strong>Definition 2</strong><br />
Let $\mathcal{F}$ be a regular functional, let $\mu \in \mathcal{P}<em>{\text{abs}}(\Omega)$, and suppose that there exists a continuous function<br />
$\frac{\delta \mathcal{F}}{\delta \mu}(\mu): \mathbb{R}^d \to \mathbb{R}$ such that, for all $\rho \in \mathcal{P}</em>{\text{abs}}(\Omega)$:</p>

\[\lim_{\epsilon \to 0} \frac{\mathcal{F}(\mu + \epsilon \chi) - \mathcal{F}(\mu)}{\epsilon} = \int \frac{\delta \mathcal{F}}{\delta \mu}(\mu) \, d\chi, \quad \chi := \rho - \mu.\]

<p>Then we call $\frac{\delta \mathcal{F}}{\delta \mu}(\mu)$ the <strong>first variation</strong> of $\mathcal{F}$ at $\mu$.</p>

<p>The first variation provides a useful necessary condition for optimality:</p>

<p><strong>Lemma 1</strong><br />
Let $\mathcal{F}$ be a regular functional which admits a first variation for all $\mu \in \mathcal{P}<em>{\text{abs}}(\Omega)$. Suppose that $\mathcal{F}$ admits a minimizer $\mu^\star \in \mathcal{P}</em>{\text{abs}}(\Omega)$. Then $\frac{\delta \mathcal{F}}{\delta \mu}(\mu^\star)$ is $\mu^\star$-almost everywhere constant.</p>

<p><em>Proof:</em><br />
If $\mu^<em>$ is a minimizer, then for any $\epsilon$ and $\chi := \rho - \mu^</em>$, we have:</p>

\[\frac{\mathcal{F}(\mu^* + \epsilon \chi) - \mathcal{F}(\mu^*)}{\epsilon} \geq 0.\]

<p>Taking the limit as $\epsilon \rightarrow 0$, we get that</p>

\[\int \frac{\delta \mathcal{F}}{\delta \mu}(\mu^*) \, d\mu^* \leq \int \frac{\delta \mathcal{F}}{\delta \mu}(\mu^*) \, d\rho\]

<p>for any $\chi$.</p>

<p>Suppose that $\frac{\delta \mathcal{F}}{\delta \mu}(\mu^\star)$ is not $\mu^\star$-almost everywhere constant. Define $Q := \int \frac{\delta \mathcal{F}}{\delta \mu}(\mu^\star) \, d\mu^\star$, and let $U := {x \in \mathbb{R}^d \mid \frac{\delta \mathcal{F}}{\delta \mu}(x) &lt; Q}$, which is nonempty by assumption. Since $\frac{\delta \mathcal{F}}{\delta \mu}(\mu^\star)$ is continuous, $U$ is measurable. By assumption, $\mu^\star(U) &gt; 0$, and since $\mu^\star$ is absolutely continuous, this implies we may find an absolutely continuous probability measure $\rho$ such that the support of $\rho$ is contained in $U$. Then:</p>

\[\int \frac{d\mathcal{F}}{d\mu}(\mu^\star) \, d\rho &lt; \int \frac{d\mathcal{F}}{d\mu}(\mu^\star) \, d\mu^\star,\]

<p>which is a contradiction. Hence $\frac{\delta \mathcal{F}}{\delta \mu}(\mu^\star)$ is $\mu^\star$-almost everywhere constant. $\blacksquare$</p>

<p>We will use the above characterization to describe the set of minimizers of $\mathcal{K}<em>{\lambda, \mathcal{V}}$. For convenience, let $\Omega$ be compact. We claim that $\mathcal{K}</em>{\lambda, \mathcal{V}}$ admits a minimizer. This requires an application of Prokhorov’s theorem:</p>

<p><strong>Theorem 1</strong> (Prokhorov’s Theorem) Let $(\mathcal{X},d)$ be a complete, compact, separable metric space. Then any sequence of probability measures ${\mu_n}_{n=1}^\infty\subseteq \mathcal{P}(\mathcal{X})$ contains a convergent subsequence (in the weak topology).</p>

<p>We remark that the compactness assumption in Theorem 1 can be dropped if the sequence of probability measures is <em>tight</em> (in a compact metric space, any ${\mu_n}_{n=1}^\infty$ is tight).</p>

<p><strong>Lemma 2</strong> Let $\Omega\subset \mathbb{R}^d$ be compact, and let $\mathcal{V}={\nu_1,..,\nu_m}\subset \mathcal{P}<em>{abs}(\Omega)$. Then for any $\lambda\in\Delta^m$, $\mathcal{K}</em>{\lambda,\mathcal{V}}$ admits an absolutely continuous minimizer.</p>

<p><em>Proof:</em><br />
As $KL(-, \nu_j)$ is nonnegative and lower-semicontinuous for all $1 \leq j \leq m$, $\mathcal{K}<em>{\lambda, \mathcal{V}}$ is as well, and $|\inf</em>{\rho \in \mathcal{P}(\Omega)} \mathcal{K}<em>{\lambda, \mathcal{V}}(\rho)| &lt; \infty$. Let $S := {\mu_n}</em>{n=1}^\infty$ be an infimizing sequence for $\mathcal{K}<em>{\lambda, \mathcal{V}}$. By Theorem 1, ${\mu_n}</em>{n=1}^\infty$ contains a subsequence ${\mu_n’}_{n=1}^\infty$, which converges to a limit point $\mu^\star \in \mathcal{P}(\Omega)$.</p>

<p>As $\mathcal{K}<em>{\lambda, \mathcal{V}}(\rho) = \infty$ for all $\rho \in \mathcal{P}(\Omega) \setminus \mathcal{P}</em>{\text{abs}}(\Omega)$, it must be the case that $S \subset \mathcal{P}<em>{\text{abs}}(\Omega)$. Furthermore, by lower semicontinuity of $\mathcal{K}</em>{\lambda, \mathcal{V}}$, we have that:</p>

\[\mathcal{K}_{\lambda, \mathcal{V}}(\mu^\star) \leq \liminf_{n \to \infty} \mathcal{K}_{\lambda, \mathcal{V}}(\mu_n').\]

<p>This shows that $\mu^\star$ minimizes $\mathcal{K}<em>{\lambda, \mathcal{V}}$, and furthermore that $\mu^\star \in \mathcal{P}</em>{\text{abs}}(\Omega)$.</p>

<p>We compute the first variation of</p>

\[\frac{\delta \mathcal{K}_{\lambda, \mathcal{V}}}{\delta \mu}(\mu) = \sum_{j=1}^m \lambda_j \log\left(\frac{d\mu}{dx}\right) - \sum_{j=1}^m \lambda_j \log\left(\frac{d\nu_j}{dx}\right)\]

\[= \log\left(\frac{d\mu}{dx}\right) + \sum_{j=1}^m \lambda_j V_j.\]

<p>Let $\mu^\star$ minimize $\mathcal{K}_{\lambda, \mathcal{V}}$. Then by Lemma 1, there exists a constant $C$ such that, for (Lebesgue) almost-every $x \in \Omega$:</p>

\[C = \log\left(\frac{d\mu^\star}{dx}(x)\right) + \sum_{j=1}^m \lambda_j V_j(x)\]

\[\Rightarrow \frac{d\mu^\star}{dx}(x) = \exp\left(C - \sum_{j=1}^m \lambda_j V_j(x)\right).\]

<p>As $\mu^\star$ is a probability measure, its density must integrate to 1, and hence we have (uniquely) identified the constant $C$ as the normalization constant</p>

\[\ln\left(\int_\Omega \exp\left(-\sum_{j=1}^m \lambda_j V_j(x)\right) dx\right).\]

<p>We have thus established the theorem:</p>

<p><strong>Theorem 2</strong> Let $\Omega\subset \mathbb{R}^d$ be compact, and let $\mathcal{V}={\nu_1,…,\nu_m}\subset \mathcal{P}<em>{abs}(\Omega)$, where for all $1\leq j\leq m$, $\nu_j\propto \exp(-V_j)$ for some continuous and nonnegative $V_j$. Then for any $\lambda\in\Delta^m$:
\begin{equation*}
\texttt{argmin}</em>{\mu\in \mathcal{P}(\Omega)}\mathcal{K}<em>{\lambda,\mathcal{V}}(\mu) =\frac{1}{Z</em>\lambda}\exp\left(-\sum_{j=1}^m\lambda_j V_j\right)
\end{equation*}
where $Z_\lambda=\int_\Omega \exp(-\sum_{j=1}^m\lambda_j V_j(x))dx$ is a normalization constant.</p>]]></content><author><name>Brendan Mallery</name></author><category term="Information Theory" /><category term="Barycenters" /><summary type="html"><![CDATA[In this post I’ll record a simple but instructive calculation, giving a basic template for characterizing optimizers of functionals on probability space: Let $\Delta^m := {(\lambda_1, \lambda_2, \dots, \lambda_m) \in \mathbb{R}^m \mid \lambda_j \geq 0,\ \sum_{j=1}^m \lambda_j = 1}$ be the $m$-dimensional probability simplex. Let $\mathcal{V} = {\nu_1, \dots, \nu_m}$ be a family of probability measures on $\Omega \subseteq \mathbb{R}^d$ such that $\nu_j \propto \exp(-V_j)$ for some nonnegative continuous functions $V_j$.]]></summary></entry><entry><title type="html">Kernel Ridge Regression</title><link href="https://brendanmallery9.github.io/blog/2024/Kernel_ridge/" rel="alternate" type="text/html" title="Kernel Ridge Regression" /><published>2024-11-24T00:00:00+00:00</published><updated>2024-11-24T00:00:00+00:00</updated><id>https://brendanmallery9.github.io/blog/2024/Kernel_ridge</id><content type="html" xml:base="https://brendanmallery9.github.io/blog/2024/Kernel_ridge/"><![CDATA[<h2 id="constructing-reproducing-kernel-hilbert-spaces">Constructing Reproducing Kernel Hilbert Spaces</h2>

<p><strong>Definition 1</strong> Let $\mathcal{X}$ be a metric space. A positive semidefinite kernel is a symmetric function $\mathcal{K}:\mathcal{X}\times \mathcal{X}\rightarrow \mathbb{R}$ such that for all ${x_i}<em>{i=1}^n\subset \mathcal{X}$, the matrix $K</em>{ij}:=\mathcal{K}(x_i,x_j)$ is PSD.</p>

<p><strong>Example 1</strong>
Let $\mathcal{X}=\mathbb{R}^d$, $\mathcal{K}(x,x’)=\langle x,x’\rangle$. Let ${x_i}<em>{i=1}^n$ be an arbitrary set of points. Then for any vector $\alpha\in \mathbb{R}^d$, we have:
\begin{equation*}
\alpha^\top K \alpha=\sum</em>{i,j=1}^n\alpha_i\alpha_j\langle x_i,x_j\rangle=|\sum_{i=1}^n \alpha_i x_i|^2\geq 0
\end{equation*}</p>

<p><strong>Example 2</strong> Let $\mathcal{X}=\mathcal{P}<em>2(\mathbb{R}^d)$. For any $\rho\in \mathcal{P}_2(\mathbb{R}^d)$, define the function $\mathcal{K}</em>\rho(\mu,\nu)=\langle Id-T_{\rho\rightarrow \mu}, Id-T_{\rho\rightarrow \nu}\rangle <em>{L^2(\rho)}$. Then for any $\mathcal{V}:={\nu_1,…,\nu_m}$, $K</em>\mathcal{V}$ is PSD.</p>

<h2 id="function-interpolation">Function Interpolation</h2>

<p>We want to solve the following problem:</p>

<p><strong>Problem 1</strong> (Minimal Norm Interpolation) Let $\mathcal{D}:={(x_i,y_i)}<em>{i=1}^n$ be data-response pairs, where $x_i\in \mathcal{X}$ and $y_i\in \mathbb{R}$. Let $\mathcal{A}</em>\mathcal{D}\subset \mathcal{H}<em>\mathcal{K}$ be the set of functions such that $f(x_i)=y_i$ for all $1\leq i\leq n.$ Choose $\bar{f}\in \texttt{argmin}</em>{f\in \mathcal{A}<em>\mathcal{D}}|f|</em>{\mathcal{H}_\mathcal{K}}$.</p>

<p>In the RKHS case, this problem is greatly simplified.</p>

<p><strong>Proposition 1</strong> 
Let $\mathcal{K}$ be a kernel on $\mathcal{X}$, let $K\in \mathbb{R}^{n\times n}$ be the matrix defined
\begin{equation<em>}
K_{ij}=\frac{1}{n}\mathcal{K}(x_i,x_j).
\end{equation</em>}
Then Problem 1 is feasible iff $y=(y_1,…,y_n)\in \text{range}(K)$. If this holds, then the optimal solution can be written:
\begin{equation<em>}
\hat{f}(-)=\frac{1}{\sqrt{n}}\sum_{i=1}^n \hat{\alpha}_i \mathcal{K}(-,x_i), \; \; \; \; K\hat{\alpha}=\frac{y}{\sqrt{n}}
\end{equation</em>}
where $\hat{\alpha}\in \mathbb{R}^n.$</p>

<p><em>Proof:</em></p>

<p>For any vector $ \alpha \in \mathbb{R}^n $, define</p>

\[\mathcal{F} := \left\{ f_\alpha(-) := \frac{1}{\sqrt{n}} \sum_{i=1}^n \alpha_i \mathcal{K}(-, x_i) \right\}.\]

<p>For any $ f_\alpha \in \mathcal{F} $, we have:</p>

\[f_\alpha(x_j) = \frac{1}{\sqrt{n}} \sum_{i=1}^n \alpha_i \mathcal{K}(x_j, x_i) = \sqrt{n} [K\alpha]_j,\]

<p>i.e., $ \sqrt{n} $ times the $ j $-th entry of $ K\alpha $. Hence, $ f_\alpha \in \mathcal{F} $ solves Problem 1 if and only if</p>

\[K\alpha = \frac{y}{\sqrt{n}}.\]

<p>We conclude by showing that the solution must be in $ \mathcal{F} $: Given any $ f \in \mathcal{H}_\mathcal{K} $, we can decompose</p>

\[f = f_\alpha + f_\perp,\]

<p>where $ f_\alpha \in \mathcal{F} $ and $ f_\perp \in \mathcal{F}^\perp $ (since $ \mathcal{F} $ is finite-dimensional and hence closed, this decomposition is always possible). By the reproducing property, we have:</p>

\[f(x_j) = \langle f, \mathcal{K}(-, x_j) \rangle_{\mathcal{H}_\mathcal{K}} = \langle f_\alpha + f_\perp, \mathcal{K}(-, x_j) \rangle_{\mathcal{H}_\mathcal{K}} = f_\alpha(x_j),\]

<p>since ( \mathcal{K}(-, x_j) \in \mathcal{F} ) and is hence orthogonal to ( f_\perp ). This, along with the fact that</p>

\[\|f_\alpha + f_\perp\|^2_{\mathcal{H}_\mathcal{K}} = \|f_\alpha\|^2_{\mathcal{H}_\mathcal{K}} + \|f_\perp\|^2_{\mathcal{H}_\mathcal{K}},\]

<p>implies that the solution must lie in ( \mathcal{F} ), if it exists. \blacksquare</p>

<p>We now consider a harder problem: Suppose we observe $n$-i.i.d. data points $y_i=f^*(x_i)+w_i$ where $f$ is some function of data $x_i$ and $w=(w_1,…,w_n)$ is a noise vector. We will solve this via a relaxation of the previous method:</p>

<p><strong>Problem 2</strong>
(Kernel ridge regression) For some $\lambda_n&gt;0$, define, find:</p>

<p>\begin{equation<em>} \hat{f}=\texttt{argmin}_{f\in \mathcal{H}_\mathcal{K}}\left{\frac{1}{2n}\sum_{i=1}^n(y_i-f(x_i))^2+\lambda_n |f|^2_{\mathcal{H}_\mathcal{K}}\right}\end{equation</em>}</p>

<p><strong>Proposition 2</strong>
For all $\lambda_n&gt;0$, Problem 2 can be written as:
\begin{equation}\label{eqn:krr solution}
\hat{f}(-)=\frac{1}{\sqrt{n}}\sum_{i=1}^n\hat{\alpha}_i\mathcal{K}(-,x_i),
\end{equation}
where the optimal weight vector $\hat{\alpha}\in \mathbb{R}^n$ is given by:
\begin{equation<em>}
\hat{\alpha}=(K+\lambda_n I_n)^{-1}\frac{y}{\sqrt{n}}.
\end{equation</em>}</p>

<p><em>Proof:</em>
The formula for the optimal $\hat{f}$ follows the same reasoning as in the previous proof. To compute the optimal weight vector, first notice that for any function $f$ of the above form, for each $j = 1, 2, …, n$ we have:</p>

\[f(x_j) = \frac{1}{\sqrt{n}} \sum_{i=1}^n \alpha_i \mathcal{K}(x_j, x_i) = \sqrt{n} \, e_j^\top K \alpha,\]

<p>where $e_j$ is a basis vector and $K_{ij} = \mathcal{K}(x_i, x_j)/n$. Similarly, we have:</p>

\[\|f\|^2_{\mathcal{H}_\mathcal{K}} = \frac{1}{n} \left\langle \sum_{i=1}^n \alpha_i \mathcal{K}(-, x_i), \sum_{j=1}^n \alpha_j \mathcal{K}(-, x_j) \right\rangle_{\mathcal{H}_\mathcal{K}} = \alpha^\top K \alpha.\]

<p>Plugging these into the cost function we get:</p>

\[\frac{1}{2n} \sum_{i=1}^n \left( y_i - \sqrt{n} [K \alpha]_i \right)^2 + \lambda_n \|f\|^2_{\mathcal{H}_\mathcal{K}} = \frac{1}{n} \|y - \sqrt{n} K \alpha\|^2_2 + \lambda \alpha^\top K \alpha,\]

<p>which expands to:</p>

\[\frac{1}{n} \|y\|^2_2 + \alpha^\top (K^2 + \lambda K) \alpha - \frac{2}{\sqrt{n}} y^\top K \alpha.\]

<p>Taking the gradient with respect to $\alpha$ and setting it equal to zero, we get:</p>

\[K(K + \lambda I_n)\alpha - \frac{1}{\sqrt{n}} K y = 0,\]

<p>which implies:</p>

\[K(K + \lambda I_n)\alpha = \frac{1}{\sqrt{n}} K y.\]

<p>Solving for $\alpha$ gives us our solution.</p>

<p>\blacksquare</p>]]></content><author><name>Brendan Mallery</name></author><category term="Convex Optimization" /><category term="Dictionary Learning" /><category term="Regularization" /><summary type="html"><![CDATA[Constructing Reproducing Kernel Hilbert Spaces]]></summary></entry><entry><title type="html">Convexity and Ricci Curvature</title><link href="https://brendanmallery9.github.io/blog/2024/ollivier_curvature/" rel="alternate" type="text/html" title="Convexity and Ricci Curvature" /><published>2024-10-07T00:00:00+00:00</published><updated>2024-10-07T00:00:00+00:00</updated><id>https://brendanmallery9.github.io/blog/2024/ollivier_curvature</id><content type="html" xml:base="https://brendanmallery9.github.io/blog/2024/ollivier_curvature/"><![CDATA[<p>There is a very rich connection between convexity and notions of curvature for Markov processes, which has given rise to a number of fruitful analogies. A key part of these analogies is the following idea: Just as (strict) convexity is often just the right condition to guarantee (fast) convergence of a gradient flow to a minima, (positive) curvature for a Markov process is the right condition to guarantee (fast) convergence of a Markov process to equilibrium. What’s more, these conditions also ensure uniqueness of the limiting object for these dynamical systems. In this post, we will outline an elementary instance of this connection for the notion of Ollivier Ricci curvature.</p>

<p><strong>Convexity</strong></p>

<p>Recall that a function $f:\mathbb{R}^d\rightarrow \mathbb{R}$ is $\alpha$-strictly convex (for $\alpha\geq 0$) if it satisfies the following inequality:</p>

\[f(x)-f(y)\geq \langle \nabla f(y),x-y\rangle +\frac{\alpha}{2}\|y-x\|^2\]

<p>for all $x$ and $y$. Observe that a $0$-strictly convex function is simply a convex function. Strict convexity is a strong condition for a function to satisfy, and has a number of important consequences. From an optimization perspective, the two that stand out are:</p>

<ol>
  <li>
    <p>Strictly convex functions have a unique minimum, and;</p>
  </li>
  <li>
    <p>It is easy to find this minimum by following the gradient.</p>
  </li>
</ol>

<p>The first property is trivial to verify: Observe that if there were two minima $x_1^\star$ and $x_2^\star$, the above inequality reduces to 
  \(0\geq \langle \nabla f(x_1^\star),x_1^\star-x_2^\star\rangle +\frac{\alpha}{2}\|x_1^\star-x_2^\star\|^2\)</p>

<p>and since $\nabla f(x_1^\star)=0$ we have $x_1^\star=x_2^\star$. The second property can be proved in both discrete and continuous time, where in the first case, “following the gradient” means gradient descent, i.e. setting $x_t=x_{t-1}-\eta\nabla f(x_{t-1})$ for some initialization $x_0$ and stepsize $\eta&gt;0$. The continuous time analog is gradient flow, which is defined by the differential equation 
  \(\dot{x}_t=-\nabla f(x_t),\)</p>

<p>with some initial condition $x_0$. The discrete case is more relevant for this analogy, so we give the standard proof here. For convenience we assume $f$ is L-Lipschitz, in which case we can set $\eta=\frac{1}{L}$. Let $x_0\in \mathbb{R}^d$, and let $x_\star=\min_{y\in \mathbb{R}^d} f(y)$. Then:</p>

\[\|x_{t+1}-x_\star\|^2_2 = \|x_t-x_\star-\eta \nabla f(x_t)\|^2_2\]

\[=\|x_t-x_\star\|^2_2-2\eta\langle \nabla f(x_t),x_t-x_\star\rangle +\eta^2 \|\nabla f(x_t)\|^2_2\]

<p>By $\alpha$-strong convexity and $L$-smoothness, we may upper bound this by</p>

\[\leq (1-\eta \alpha)\|x_t-x_\star\|^2_2-2\eta(f(x_t)-f(x_\star))+2\eta^2 L(f(x_t)-f(x_\star))\]

\[=(1-\eta \alpha)\|x_t-x_\star\|_2^2-2\alpha(1-\alpha L) (f(x_t)-f(x_\star))\]

\[=(1-\eta\alpha)\|x_t-x_\star\|^2_2\]

<p>Iterating this, we get that:</p>

\[\|x_t-x_\star\|_2^2\leq (1-\eta\alpha)^t\|x_0-x_\star\|_2^2\]

<p>which implies that $x_t$ converges to the solution in $|.|_2^2$ exponentially fast. We make two remarks:</p>
<ol>
  <li>If we remove the smoothness assumption from $f$, one may still conclude that (a suitably generalized version of) gradient descent will converge provided one appropriately shrinks the step-size at each iteration (otherwise one is left with an error proportional to $\eta$).</li>
  <li>The same proof holds for all $0\leq \eta \leq \frac{1}{L}$.</li>
</ol>

<p>The first part to our analogy is that the exponential rate of convergence is governed by the strength of convexity $\alpha$: if we ignore $\eta$ by setting it equal to $1$ (hence assuming that $f$ is 1-Lipschitz), then we can consider $\mathbb{R}^d$ under the dynamical system defined by mapping</p>

\[T:\mathbb{R}^d\rightarrow \mathbb{R}^d\]

\[T(x) =x-\nabla f(x).\]

<p>for $f$ an $\alpha$-strongly convex function. Then $T^t$ contracts $\mathbb{R}^d$ by a factor of $(1-\alpha)^t$. Thus we may view $\alpha$ as the coefficient of contraction for the self map of $\mathbb{R}^d$ induced by the (discrete) gradient flow of $f$. Over many applications of the map, the entire metric space contracts to a point $x_\star$ which is the minima for $f$. This also constitutes a proof of the uniqueness of the minimal value for $f$.</p>

<p><strong>Ricci Curvature</strong></p>

<p><strong>Definition:</strong> (Ollivier Ricci Curvature) Ollivier Ricci curvature, which is a notion of curvature that is defined for a Markov chain on a metric space $X$. Let $M:x\rightarrow \mathcal{P}(X)$ be Markov kernel, i.e. a measurable map from $X$ to $\mathcal{P}(X)$, the set of probability measures on $X$. Then the Ollivier Ricci curvature of $M$ is:</p>

\[\kappa(x,y)=\frac{d(x,y)-W_1(M(x),M(y))}{d(x,y)}.\]

<p>where $d$ is the metric on $X$ and $W_1$ is the 1-Wasserstein, or optimal transport distance on $\mathcal{P}(X)$.</p>

<p>How should one think about this curvature? It is most natural to think about $M(x)$ as a probability measure centered at $x$ (for instance the uniform measure on a ball centered with $x$ at its center). Then one can think of $M$ as defining a density of particles around each point in the metric space. Then $\kappa(x,y)$ can be seen as a comparison between the distance a particle at $x$ must travel to reach $y$ versus the “average” distance a particle nearby $x$ must travel to reach a particle nearby $y$. If $\kappa(x,y)\geq 0$, then a random nearby particle must on average travel less distance than the one placed at $x$, whereas if $\kappa(x,y)\leq 0$, the nearby particle must travel further. Thus $\kappa(x,y)$ says something about the “local geometry” of the pair of points $(x,y)$.</p>

<p>The easiest way to convince yourself that this is a good notion of curvature is by example. For the following examples, we take $M$ to be the uniform measure on a ball of some radius $r&gt;0$ around each point in $X$:</p>
<ul>
  <li>The quintessential positively curved space, a sphere, has everywhere positive Ollivier Ricci curvature. This is easiest to see when $x$ and $y$ are antipodal points on the sphere. In this case, $M(x)$ and $M(y)$ are small “caps” on the ends of the sphere, and one should have no problem believing that $W_1(M(x),M(y))&lt;d(x,y)$.</li>
  <li>“Tree-like” metric spaces tend to have negative curvature. Let $\mathcal{T}$ infinite tree with minimum degree at least $3$, equipped with the graph metric. Let $x$ and $y$ be two points which are separated by at least 3 edges. Then one can easily compute that the distance between $x$ and $y$ is less than the average distance between a point in the support of $M(x)$ and the support of $M(y)$. If one is familiar with $\delta$-hyperbolicity, or the fact that an infinite tree embeds quasi-isometrically into hyperbolic space, one will be satisfied with the definition in this case.</li>
  <li>The natural example of a flat space is Euclidean space. As we have already mentioned in the case of $\mathbb{R}^2$, $W_1(M(x),M(y))=d(x,y)$ for two balls $M(x)$ and $M(y)$, centered at $x$ and $y$ respectively, hence $\kappa(x,y)=0$.</li>
</ul>

<p>In all of the above, the dependence on a Markov chain is avoided to highlight the geometric nature of $\kappa$. But our main interest in $\kappa$ comes from the following theorem, which equates positive curvature of $M$ with rapid convergence of the Markov process generated by $M$: For any Markov kernel $M$ and probability measure $\mu$, we denote $M\star\mu$ as the convolution of $\mu$ by $M$.</p>

<p><strong>Theorem</strong> Let $(X,d)$ be a metric space and $M$ a Markov kernel on $X$. Then $\kappa(x,y)&gt;K&gt;0$ for all $x,y\in X$ iff $W_1(M\star\mu,M\star\nu)\leq (1-\alpha)W_1(\mu,\nu)$ for all $\mu,\nu\in \mathcal{P}(X)$.</p>

<p>The theorem was first discovered (as far as I can tell) in the context of path coupling by <a href="https://www.math.cmu.edu/~af1p/Teaching/Markov_Chain_Mixing/Papers/Pathcouple.pdf">Bubbly and Dyer</a>, and hence predates the definition of $\kappa$. To rephrase it in a way more suggestive in our analogy, convolution by $M$ defines a map</p>

\[M\star:\mathcal{P}(X)\rightarrow \mathcal{P}(X)\]

<p>sending $\mu$ to $M\star\mu$. Then if $(M\star)^t$ has curvature bounded below by $\alpha&gt;0$, $M\star$ is a contraction, and in particular contracts $(\mathcal{P}(\Omega),W_1)$ by a factor of $(1-\alpha)^t$. Over many applications of $M\star$, the entire space $\mathcal{P}(\Omega)$ contracts to the stationary distribution of $M\star$, which is necessarily unique.</p>

<p><strong>Convexity meets curvature</strong></p>

<p>We’ve seen that both $\alpha$-convexity for a function $f$ and $\kappa&gt;\alpha$ for a Markov kernel imply contraction properties for dynamical systems associated to these objects. These two properties come together in a very nice way when considering Langevin diffusion. Langevin diffusion is a stochastic process on $\mathbb{R}^d$, governed by the following stochastic differential equation:</p>

\[dX_t=-\nabla V(X_t)dt+dB_t\]

<p>where $V:\mathbb{R}^d\rightarrow\mathbb{R}$ is a potential function and $B_t$ is a standard Brownian motion. One can think of $X_t$ as the trajectory of a particle undergoing random motion in a force field $\nabla V$. As $t\rightarrow \infty$, the law of $X_t$ converges to a unique stationary distribution, given by the Gibbs measure $\mu_{\star}= \frac{1}{Z}e^{-V}$, with the normalizing constant $Z=\int_{\mathbb{R}^d} e^{-\nabla V(x)}dx$. The rate at which the process converges to its stationary distribution is a well-studied topic, as it suggests natural sampling schemes for distributions that can be written in the form $\frac{1}{Z}e^{-V}$. In particular, if one assumes that $V$ is $\alpha$-strongly convex, one is guaranteed exponential convergence of the Langevin diffusion to $\mu_\star$. This can be proved directly in a fairly straightforward manner, but we will take a nonstandard perspective on this, which was first made clear to me by the aforementioned blog post.</p>

<p>Consider the Euler-Marayama discretization of $X_t$, with $k+1$’st iterate:
  \(x^{k+1}=x^{k}-\frac{\delta}{2}\nabla V(x^k) + \sqrt{\delta}\zeta\)</p>

<p>for $\zeta$ distributed according to a standard normal $\mathcal{N}(0,1)$ and $\delta&gt;0$. This gives rise to a Markov chain, and so we may try to compute its Ollivier Ricci curvature. Let $x_0,y_0$ be two different initial points in $\mathbb{R}^d$, and let $M(x)=\mu_1^x$ and $M(y)=\mu_1^y$ be the laws of $x_1$ and $y_1$ respectively. Then we have:</p>

\[\mu_1^x=\mathcal{N}(x_0,\delta)\star\delta_{x_0-\frac{\delta}{2}\nabla V(x_0)}\]

\[\mu_1^y=\mathcal{N}(y_0,\delta)\star\delta_{y_0-\frac{\delta}{2}\nabla V(y_0)}\]

<p>We may couple these two measures by pushing $\mu_1^x$ forward onto $\mu_1^y$ by translation by the vector $y_0-\frac{\delta}{2}\nabla V(y_0)-(x_0-\frac{\delta}{2}\nabla V(x_0))\in \mathbb{R}^d$, and hence we may upper bound $W_1(\mu_1^x,\mu^y_1)$ as follows:</p>

\[W_1(\mu_1^x,\mu_1^y)\leq \|x_0-y_0-\frac{\delta}{2}(\nabla V(x_0)-\nabla V(y_0))\|\]

<p>If we now assume that $V$ is $\alpha$-strongly convex, we may upper bound this quantity by:</p>

\[\|(x_0-y_0)\left(1-\frac{\delta}{2}\alpha\right)\|\]

<p>Plugging this bound into our formula for the curvature between $x_0$ and $y_0$, we have: 
  \(\kappa(x_0,y_0)\geq \frac{\|x_0-y_0\|-\|(x_0-y_0)(1-\frac{\delta}{2}\alpha)\|}{\|x_0-y_0\|}\geq \frac{\delta}{2}\alpha.\)</p>

<p>As this is true for all initial points $x_0,y_0$, we have a global positive curvature bound of $\frac{\delta\alpha}{2}$, which guarantees that the Euler-Marayama discretization of $X_t$ converges to equilibrium at a rate of at least $(1-\frac{\delta}{2}\alpha)^t$. Thus we see that the convexity of $V$ directly implies a positive curvature bound for (a discrete approximation to) the Langevin diffusion with potential $V$, and together they imply an exponential rate of convergence for this process.</p>

<p><strong>Concluding remarks</strong></p>

<p>As I mentioned previously, proving the continuum analog of the above result is not very difficult, but its worth noting that the two arguments are essentially identical in spirit. The key to both is coupling two independent copies of the process initialized at different points, and then concluding by a curvature bound (in the discrete case) or Gronwall’s inequality (in the continuum case).</p>

<p>Ollivier Ricci curvature is particularly convenient for discrete time Markov processes on discrete spaces. For continuum time and space Markov processes, there are other more suitable curvatures, including Bakry-Emry curvature and the synthetic Ricci curvature due to Lott, Villani and others, which I will perhaps talk about in a future post. These notions of curvature are theoretically richer than Ollivier Ricci curvature, and require a greater technical overhead. But the general idea remains the same between all notions, that a positive lower bound on curvature results in quantitative contraction in the Wasserstein distance.</p>]]></content><author><name>Brendan Mallery</name></author><category term="Optimal Transportation" /><category term="Ricci Curvature" /><category term="Sampling" /><category term="Gradient Flows" /><summary type="html"><![CDATA[There is a very rich connection between convexity and notions of curvature for Markov processes, which has given rise to a number of fruitful analogies. A key part of these analogies is the following idea: Just as (strict) convexity is often just the right condition to guarantee (fast) convergence of a gradient flow to a minima, (positive) curvature for a Markov process is the right condition to guarantee (fast) convergence of a Markov process to equilibrium. What’s more, these conditions also ensure uniqueness of the limiting object for these dynamical systems. In this post, we will outline an elementary instance of this connection for the notion of Ollivier Ricci curvature.]]></summary></entry><entry><title type="html">How to Reverse an SDE</title><link href="https://brendanmallery9.github.io/blog/2024/reverseSDEs/" rel="alternate" type="text/html" title="How to Reverse an SDE" /><published>2024-10-07T00:00:00+00:00</published><updated>2024-10-07T00:00:00+00:00</updated><id>https://brendanmallery9.github.io/blog/2024/reverseSDEs</id><content type="html" xml:base="https://brendanmallery9.github.io/blog/2024/reverseSDEs/"><![CDATA[<p>The following derivation is borrowed from <d-cite key="sarkka2019applied"></d-cite>.</p>

<h2 id="the-fokker-planck-equation">The Fokker-Planck Equation</h2>

<p>Let 
 \(X_t = b_t(X_t)dt + \sigma_t(X_t)dB_t, \; \; \; \; X_0 \sim \mu_0\)</p>

<p>be a stochastic differential equation, where $b_t \in \mathbb{R}^d$ and $\sigma_t \in \mathbb{R}^{d \times d}$. The Fokker-Planck Equation gives us a way to express the density of $X_t$ as a PDE:</p>

<p><strong>Theorem (Fokker-Planck Equation):</strong> Let $\mu_t = \text{Law}(X_t)$ and $p_t$ be the density of $\mu_t$. Then $p_t$ solves:</p>

\[\partial \mu_t(x) = - \sum_i \frac{\partial}{\partial x_i} \left( [b_t(x)]_i \mu_t(x) \right) + \frac{1}{2} \sum_{i,j} \frac{\partial^2}{\partial x_i \partial x_j} \left( [\sigma_t(x)\sigma_t^\top(x)]_{ij} \mu_t(x) \right)\]

\[= - \text{div}(b_t(x) \mu_t(x)) + \frac{1}{2} \sum_{i,j} \frac{\partial^2}{\partial x_i \partial x_j} \left( [\sigma_t(x) \sigma_t^\top(x)]_{ij} \mu_t(x) \right)\]

<p><strong>Example (Spatially Homogeneous Diffusion):</strong> If $\sigma_t(x)$ is constant as a function of $x$, then this can be written as:</p>

\[\partial \mu_t(x) = - \text{div}(b_t(x) \mu_t(x)) + \langle \sigma_t \sigma_t^\top, \nabla^2 \mu_t(x) \rangle\]

\[= - \text{div}(b_t(x)\mu_t(x)) + \sigma_t \sigma_t^\top \nabla \mu_t(x)\]

<p><strong>Example (Langevin Diffusion):</strong> Let $V: \mathbb{R}^d \rightarrow \mathbb{R}$ be continuously differentiable. Then the Fokker-Planck equation is: \(\partial \mu_t(x) = \text{div}(\nabla V \mu_t) + \frac{1}{2} \Delta \mu_t\) This has SDE form: \(X_t = -\nabla V(X_t) dt + dB_t\) The SDE may be interpreted as a stochastic gradient flow for the function $V$.</p>

<p><em>Proof:</em> Let $\phi \in \mathcal{C}_c^2(\mathbb{R}^d)$. The Ito differential of $\phi(X_t)$ is given by:</p>

\[d\phi(X_t) = \langle \nabla \phi(X_t), b_t(X_t) \rangle dt + \frac{1}{2} \langle \nabla^2 \phi(X_t), \sigma_t \sigma_t^\top \rangle dt + \langle \sigma_t^\top \nabla \phi(X_t), dB_t \rangle\]

<p>Taking expectations and formally dividing both sides by $dt$ gives:</p>

\[\frac{d E[\phi]}{dt} = \mathbb{E}[\langle \nabla \phi(X_t), b_t(X_t) \rangle] + \frac{1}{2} \mathbb{E}[\langle \nabla^2 \phi(X_t), \sigma_t \sigma_t^\top \rangle]\]

<p>The left-hand side can be rewritten as:</p>

\[\frac{d E[\phi]}{dt} = \frac{d}{dt} \int \phi(x) \mu_t(x) dx = \int \phi(x) \frac{\partial \mu_t(x)}{\partial t} dx\]

<p>Recall multidimensional integration by parts:</p>

\[\int_\Omega \frac{\partial u(x)}{\partial x_i} v(x) dx = \int_{\partial \Omega} u(x) v(x) n_i dS - \int_\Omega u(x) \frac{\partial v(x)}{\partial x_i} dx\]

<p>Consider the first term in the equation above, written in coordinate form:</p>

\[\mathbb{E} \left[ \frac{\partial \phi}{\partial x_i} (X_t) [b_t(X_t)]_i \right] = \int \frac{\partial \phi}{\partial x_i} (x) [b_t(x)]_i \mu_t(x) dx = - \int \phi(x) \frac{\partial}{\partial x_i} \left( [b_t(x)]_i \mu_t(x) \right) dx\]

<p>Applying integration by parts, and similarly for the second term:</p>

\[\mathbb{E} \left[ \langle \nabla^2 \phi(X_t), \sigma_t \sigma_t^\top \rangle \right] = \int \left( \frac{\partial^2 \phi}{\partial x_i \partial x_j} \right) [\sigma_t(x) \sigma_t^\top(x)]_{ij} \mu_t(x) dx\]

\[= \int \phi(x) \frac{\partial^2}{\partial x_i \partial x_j} \left( [\sigma_t(x) \sigma_t^\top(x)]_{ij} \mu_t(x) \right) dx\]

<p>Substituting these into the earlier equation:</p>

\[\int \phi(x) \partial \mu_t(x) dx = - \sum_i \int \phi(x) \frac{\partial}{\partial x_i} \left( [b_t(x)]_i \mu_t(x) \right) dx\]

\[+ \frac{1}{2} \sum_{i,j} \int \phi(x) \frac{\partial^2}{\partial x_i \partial x_j} \left( [\sigma_t(x) \sigma_t^\top(x)]_{ij} \mu_t(x) \right) dx\]

<p>which can be rewritten as: 
\(\int \phi(x) \left( \frac{\partial \mu_t(x)}{\partial t} + \sum_i \frac{\partial}{\partial x_i} \left( [b_t(x,t)]_i \mu_t(x) \right) - \frac{1}{2} \sum_{i,j} \frac{\partial^2}{\partial x_i \partial x_j} \left( [\sigma_t(x)\sigma_t^\top(x)]_{ij} \mu_t(x) \right) \right) dx = 0\)</p>

<p>This holds for all $\phi \in \mathcal{C}_c^2(\mathbb{R}^d)$, implying:</p>

\[\frac{\partial \mu_t(x)}{\partial t} + \sum_i \frac{\partial}{\partial x_i} \left( [b_t(x,t)]_i \mu_t(x) \right) - \frac{1}{2} \sum_{i,j} \frac{\partial^2}{\partial x_i \partial x_j} \left( [\sigma_t(x)\sigma_t^\top(x)]_{ij} \mu_t(x) \right) = 0\]

<p>for $\mu_t$ almost every $x$.</p>

<h2 id="time-reversal">Time-reversal</h2>

<p>We will now construct the time reversal of the SDE. Let $X^\leftarrow$ be defined such that $Law(X_t^\leftarrow) = \pi_{T-t}$. For convenience, assume $\sigma_t$ is independent of the spatial variable. The Fokker-Planck equation gives:</p>

\[\partial \mu_t^\leftarrow = - \frac{1}{2} \langle \sigma_{T-t} \sigma_{T-t}^\top, \nabla^2 \mu_t^\leftarrow \rangle + \text{div}(\mu_t^\leftarrow b_{T-t})\]

<p>Observe that:</p>

\[\langle \sigma_{T-t} \sigma_{T-t}^\top, \nabla^2 \mu_t^\leftarrow \rangle = \text{div}(\sigma_{T-t} \sigma_{T-t}^\top \nabla \mu_t^\leftarrow)\]

<p>Substituting, the equation becomes: 
\(\partial \mu_t^\leftarrow = \frac{1}{2} \langle \sigma_{T-t} \sigma_{T-t}^\top, \nabla^2 \mu_t^\leftarrow \rangle + \text{div}(\mu_t^\leftarrow(b_{T-t} - \sigma_{T-t} \sigma_{T-t}^\top \nabla \ln \mu_t^\leftarrow))\)</p>

<p>The corresponding SDE is: 
\(dX_t^\leftarrow = \left\{ -b_{T-t}(X_t^\leftarrow) + \sigma_{T-t} \sigma_{T-t}^\top \nabla \ln \mu_t^\leftarrow(X_t^\leftarrow) \right\} dt + \sigma_{T-t} dB_t\)</p>

<p>If initialized at $X_0^\leftarrow \sim \pi_T$, then $X_t^\leftarrow \sim \pi_{T-t}$ for all $t$.</p>]]></content><author><name>Brendan Mallery</name></author><category term="Sampling" /><category term="Diffusion Models" /><summary type="html"><![CDATA[The following derivation is borrowed from .]]></summary></entry><entry><title type="html">Notes on Transportation Inequalities</title><link href="https://brendanmallery9.github.io/blog/2024/Transport-inequalities/" rel="alternate" type="text/html" title="Notes on Transportation Inequalities" /><published>2024-10-07T00:00:00+00:00</published><updated>2024-10-07T00:00:00+00:00</updated><id>https://brendanmallery9.github.io/blog/2024/Transport-inequalities</id><content type="html" xml:base="https://brendanmallery9.github.io/blog/2024/Transport-inequalities/"><![CDATA[<p>Here are some notes from back when I was trying to really learn about transport inequalities for the first time. Most of the following is distilled from <d-cite key="gozlan2010transport"></d-cite> and <d-cite key="gozlan2015transport"></d-cite>. Before getting on, I’ll say that since then there have been a number of times where I was working hard on a problem, only to realize that what I was ``really doing’’ the whole time was establishing a transport inequality. I mention this because (in my opinion), transport inequalities seem opaque and technical when you first come across them, made all the more mysterious by the fact that a lot of the most <a href="https://cedricvillani.org/sites/dev/files/old_images//2012/08/014.OV-Talagrand.pdf">celebrated results</a> in the field are about establishing them.</p>

<p>So if you are like me and are not particularly motivated by a bunch of statements like “LSI==&gt; T2 ==&gt; T1”, rest assured that these things become unexpectedly handy, unexpectedly often.</p>

<h2 id="introduction">Introduction</h2>

<p><strong>Definition 1:</strong> (Transport Inequality) Let $\mathcal{X}$ be a Polish space equipped with a cost function $c:\mathcal{X}^2\rightarrow \mathbb{R}$ with $c(x,x)=0$ for all $x\in \mathcal{X}$. Consider a function $J(-\mid-):\mathcal{P}(\mathcal{X})^2\rightarrow [0,\infty]$ and $\alpha:[0,\infty)\rightarrow [0,\infty)$ an increasing function so that $\alpha(0)=0.$ Then $\mu\in \mathcal{P}(\mathcal{X})$ satisfies the transport inequality $\alpha(\mathcal{T}_c)\leq J$ if:</p>

\[\alpha(\mathcal{T}_c(\nu,\mu))\leq J(\nu\mid\mu), \forall \nu\in\mathcal{P}(\mathcal{X})\]

<p>We will primarily concerned with the following family of examples:</p>

<p><strong>Definition 2:</strong> (Transport-Entropy Inequality) Let $(\mathcal{X},d)$ be a metric space and let $C&gt;0$. We say that $\mu$ satisfies the inequality $T_p(C)$ if:</p>

\[W^2_p(\mu,\nu)\leq CH(\nu\mid\mu)\]

<p>where $W_p$ is the $p$-Wasserstein distance and $H$ is the relative entropy.</p>

<p>We will typically take $p=1,2$. When $p=1$, this is Definition 1 with $\alpha(x)=\frac{1}{C}x^2$. When $p=2$, this is Definition 1 with $\alpha(x)=\frac{1}{C}x$. Observe by Jensen’s inequality, $T_1(C)$ is always weaker than $T_2(C).$ A basic example is the following:</p>

<p><strong>Theorem 1:</strong> (Pinsker’s Inequality ) Let $\mu,\nu \in \mathcal{P}(\mathcal{X})$, and let $d_H$ denote the Hamming distance. Then:</p>

\[\mathcal{T}^2_{d_H}(\mu,\nu)=\|\mu-\nu\|^2_{TV}\leq \frac{1}{2}H(\nu\mid\mu)\]

<p>where $|.|_{TV}$ denotes the total-variation distance.</p>

<p>A particularly famous one was proven by Talagrand:</p>

<p><strong>Theorem 2:</strong> ($T_2$ for the Standard Gaussian) The standard Gaussian measure $\gamma$ on $\mathbb{R}^d$ satisfies $T_2(2)$, and this constant is sharp.
\end{mytheo}</p>

<p>Before proving Theorem 2, we mention some applications:</p>

<p><em>Applications of transportation inequalities:</em></p>
<ol>
  <li>If $\mu$ satisfies a transportation inequality, it is concentrated w.r.t. that cost function. $T_2$ is particularly useful in that it ‘‘tensorizes’’, allowing one to establish <em>dimension independent</em> concentration for $\mu^{\otimes n}$.</li>
  <li>Transportation inequalities can allow one to prove other functional inequalities of interest.</li>
  <li>Connections to positive curvature lower bounds</li>
</ol>

<p>We will see an example of $T_2$ implying tensorization later. First, we will elaborate on the connection to concentration, via a proof attributed to <a href="https://en.wikipedia.org/wiki/Katalin_Marton">Katalin Marton</a>:</p>

<p><strong>Theorem 3</strong> (Marton’s argument) Let $\alpha: \mathbb{R}^+\rightarrow \mathbb{R}^+$ be bijective, and suppose that $\mu\in \mathcal{P}(\mathcal{X})$ satisfies $\alpha(\mathcal{T}_{d^p})\leq H$ for $p\geq 1$. Then for all measurable $A\subset \mathcal{X}$ with $\mu(A)\geq \frac{1}{2}$, the following concentration inequality holds:</p>

\[\mu(\{x\in \mathcal{X}\mid d(x,A)\leq r\}) \geq 1-e^{-\alpha(r-r_0)},\]

<p>for all $r&gt;r_0:=\alpha^{-1}(\log(2))$. Equivalently, for all $1$-Lipschitz $f$, the following holds:</p>

\[\mu(\{f&gt;m_f+r+r_0\})\leq e^{-\alpha(r)},\;\;\;\;\;\;r\geq 0\]

<p>where $m_f$ is the median of $f$.</p>

<p><em>Proof:</em> Take $A\subset \mathcal{X}$ with $\mu(A)\geq \frac{1}{2}$, and let $B=\mathcal{X}\setminus A^r$. Consider $d\mu_A(x):=\frac{1}{\mu(A)}1_A(x) d\mu(x)$ and $d\mu_B=1_B(x)d\mu(x)$. Any coupling between $\mu_A$ and $\mu_B$ has cost lower bounded by $r$, by definition. Via the triangle inequality we have :</p>

\[\begin{align*}
r &amp; \leq \mathcal{T}_{d^p}(\mu_A,\mu_B) \\&amp; \leq \mathcal{T}_{d^p}(\mu_A,\mu)+\mathcal{T}_{d^p}(\mu_B,\mu) \\ &amp; \leq \alpha^{-1}(H(\mu_A\mid\mu)) +\alpha^{-1}(H(\mu_B\mid\mu)).
\end{align*}\]

<p>We compute:</p>

\[\begin{align*}
H(\mu_A\mid\mu) &amp; = \int \log (\frac{d\mu_A}{d\mu})d\mu_A \\ &amp; = \frac{1}{\mu(A)}\int_A \log(\frac{1}{\mu(A)})d\mu \\&amp; = -\log (\mu(A)) \\&amp;  \leq \log 2
\end{align*}\]

<p>and</p>

\[\begin{align*}
H(\mu_B\mid\mu) &amp; = \frac{1}{\mu(B)}\int_B \log(\frac{1}{\mu(B)})d\mu \\ &amp; = -\log(1-\mu(A^r))
\end{align*}\]

<p>Applying $\alpha$ and exponentiating both sides, we obtain:</p>

<p>\(\begin{equation*}
\mu(A^r)\geq 1- e^{-\alpha(r-r_0)}
\end{equation*}\)
for all $r\geq r_0$.</p>

<p>Hence we see $T_p\rightarrow$ subgaussian concentration for any $p$. Observe that from the above, <strong>there is no apparent difference between $T_2$ and $T_1$, as both imply subgaussian concentration.</strong></p>

<h2 id="proof-of-theorem-2">Proof of Theorem 2</h2>

<p>In this section we give a proof of Theorem 2. The strategy will be to prove the result in 1-D then to use <em>tensorization</em> properties of $T_2$.</p>

<p><strong>Lemma 1:</strong> Let $V: \mathbb{R}\rightarrow \mathbb{R}_{\geq 0}$, and let $\mu\in \mathcal{P}_2(\mathbb{R})$ be defined by $d\mu(x)=e^{-V(x)}dx$. Then:</p>

\[H(\nu\mid\mu)\geq \int V(T_{\mu\rightarrow \nu}(x))-V(x)-V'(x)[T_{\mu\rightarrow \nu}(x)-x] d\mu(x)\]

<p>where $T_{\mu\rightarrow \nu}$ is the optimal transport map from $\mu$ to $\nu$.</p>

<p><em>Proof:</em> By Brenier’s theorem, $T_{\mu\rightarrow \nu}$ is a monotone map. Assume that $f$ is absolutely continuous and hence we have $\nu=f\mu$. We may write</p>

\[\nu((-\infty,T_{\mu\rightarrow \nu}(x)])=\mu((-\infty,x])\]

<p>for all $x\in \mathbb{R}$. By definition of pushforwards, this means:
\(\int_{-\infty}^{T_{\mu\rightarrow \nu}(x)} f(z)e^{-V(z)} dz=\int_{-\infty}^x e^{-V(z)}dz.\)</p>

<p>Differentiating, we have:
\(f(T_{\mu\rightarrow \nu}(x))e^{-V(T_{\mu\rightarrow \nu}(x))}T'_{\mu\rightarrow \nu}(x)=e^{-V(x)}.\)</p>

<p>Taking logarithms, we get:
\(\log f(T_{\mu\rightarrow \nu}(x))+\log T'_{\mu\rightarrow \nu}(x) -V(T_{\mu\rightarrow \nu}(x))=-V(x).\)</p>

<p>Integrating both sides w.r.t. $\mu$, we get:
\(\begin{align*}
\int \log f d\nu &amp; =  \int (V(T_{\mu\rightarrow \nu}(x))-V(x) - \log T'_{\mu\rightarrow \nu}(x))e^{-V(x)}dx 
\end{align*}\)</p>

<p>By integration by parts, we have: $\int (T_{\mu\rightarrow \nu}(x)-x)V’(x) e^{-V(x)}dx=\int (T’_{\mu\rightarrow \nu}(x)-1)e^{-V(x)}dx.$ Adding and subtracting this to the above equation we get:</p>

\[\begin{align*}
\int \log f d\nu &amp; = \int (V(T_{\mu\rightarrow \nu}(x))-V(x)-V'(x)[T_{\mu\rightarrow \nu}(x)-x])d\mu(x) +\int T'_{\mu\rightarrow \nu}(x)-1 -\log T'_{\mu\rightarrow \nu}(x) d\mu(x) \\ &amp; \geq \int V(T_{\mu\rightarrow \nu}(x))-V(x)-V'(x)[T_{\mu\rightarrow \nu}(x)-x]d\mu
\end{align*}\]

<p>since $b-1-\log b\geq 0$ for all $b&gt;0$. The left hand side is $H(\nu\mid\mu)$ and we are done.</p>

<p><strong>Corollary 1:</strong> ($T_1$ for the standard 1D Gaussian) Let $\gamma$ be the standard 1-D Gaussian. Then $\gamma$ satisfies $T_2(2)$:</p>

\[H(\nu\mid\mu)\geq \frac{1}{2} W_2^2(\nu,\gamma)\]

<p><em>Proof:</em> Applying Lemma 1 with $V(x)=x^2/2+\log(2\pi)/2$ we get:
\(H(\nu\mid\gamma)\geq \int \frac{(T_{\gamma\rightarrow \nu}(x)-x)^2}{2} d\gamma(x) = \frac{W_2^2(\nu,\gamma)}{2}.\)</p>

<p>We remark that the general case may be proven by generalizing the above argument. Instead, we proceed by establishing the tensorization property for transport inequalities. First we define infimum convolution:</p>

<p><strong>Definition:</strong> Let $\alpha_1,\alpha_2 : [0,\infty)\rightarrow [0,\infty)$. Then the infimum convolution of $\alpha_1,\alpha_2$ is defined:
\(\alpha_1 \square \alpha_2(t)=\inf \{\alpha_1(t_1)+\alpha_2(t_2):t_1+t_2=t\}.\)</p>

<p><strong>Remark:</strong></p>
<ul>
  <li>Infimal-convolution is integral convolution in the min-plus algebra</li>
  <li>Infimum-convolution preserves convexity.</li>
  <li>If $\alpha_1,\alpha_2$ are increasing then $\alpha_1\square \alpha_2$ is as well.</li>
  <li>The proximal operator $\text{prox}^\mu_f(x)=\inf_w f(x)+\frac{1}{2\mu}|x-w|^2$ is an example of an infimal-convolution.</li>
  <li>
    <p>Note we may write Kantorovich duality as:
\(W_p(\mu,\nu)^p=\displaystyle\sup_{f \in C_b(\mathcal{\mathbb{R}^d})} \int f\square \|.\|^p d\nu - \int f d\mu.\)</p>
  </li>
  <li><strong>Important:</strong> For convex, increasing $\alpha$, we have: $\alpha^{\square n}(t)=n \alpha (t/n)$ for all $t\geq 0$. (Proof: Set $n=2$ and take a derivative, then generalize.)</li>
</ul>

<p><strong>Theorem 4:</strong> Suppose $\mu$ verifies $T_1(C)$ on $(\mathcal{X},d)$. Then $\mu^{\otimes n}$ verifies the inequality $T_1(nC)$ on $\mathcal{X}^n$ equipped with $d_1^n(x,y):=\sum_{i=1}^n d(x_i,y_i)$. Consequently, for all $1$-Lipschitz functions $f$:</p>

\[n \alpha\left(\frac{\mathcal{T}_{c^{\oplus n}} (\nu,\mu^{\otimes n})}{n}\right) \leq H(\nu\mid\mu^{\otimes n}),\]

<p>for all $\nu\in \mathcal{P}(\mathcal{X}^n)$, where $c^{\oplus n}(x,y)=\sum_{i=1}^n c(x_i,y_i)$.</p>

<p><em>Proof:</em> We will prove this for $n=2$, and the result extends to arbitrary $n$ by induction: Let $\nu\in \mathcal{P}(\mathcal{X}\times \mathcal{X})$. Then we may define the conditional expectation w.r.t. the first coordinate as follows:
\(d\nu(x_1,x_2)=d\nu_1(x_1)d\nu_2^{x_1}(x_2).\)</p>

<p>With this notation established, we establish some useful facts:
\(\begin{equation}\mathcal{T}_{c_1\otimes c_2}(\nu,\mu_1\otimes \mu_2)\leq \mathcal{T}_{c_1}(\nu_1,\mu_1)+\int \mathcal{T}_{c_2}(\nu_2^{x_1},\mu_2)d\nu_1(x_1),\label{eqn:transport_tensor}
\end{equation}\)</p>

\[\begin{equation}H(\nu\mid\mu_1\otimes \mu_2)=H(\nu_1\mid\mu_1)+\int H(\nu_2^{x_1}\mid\mu_2)d\nu_1(x_1)\label{eqn:entropy_tensor}
\end{equation}\]

<p>(\ref{eqn:transport_tensor}) is established via the coupling that transports ‘‘optimally on slices’’ of $\nu$ and $\mu_1\otimes \mu_2$. (\ref{eqn:entropy_tensor}) is a famous property known as the disintegration of entropy. We compute:</p>

\[\begin{align*}
\alpha_1\square \alpha_2 (\mathcal{T}_{c_1\oplus c_2}(\nu,\mu_1\otimes\mu_2) &amp;\leq \alpha_1\square \alpha_2\left(\mathcal{T}_{c_1}(\nu_1,\mu_1)+\int \mathcal{T}_{c_2}(\nu_2^{x_1},\mu_2)d\nu_1(x_1)\right)\\ &amp;\leq \alpha_1(\mathcal{T}_{c_1}(\nu_1,\mu_1))+\alpha_2\left(\int \mathcal{T}_{c_2}(\nu_2^{x_1},\mu_2)d\nu_1(x_1)\right) \\ &amp; \leq \alpha_1(\mathcal{T}_{c_1}(\nu_1,\mu_1))+\int \alpha_2(\mathcal{T}_{c_2}(\nu_2^{x_1},\mu_2))d\nu_1(x_1)\\&amp; \leq H(\nu_1\mid\mu_1)+\int H(\nu_2^{x_1}\mid\mu_2)d\nu_1(x_1) \\&amp; = H(\nu\mid\mu_1\otimes \mu_2)
\end{align*}\]

<p>where we used the fact that $\alpha_1\square \alpha_2$ is increasing, the definition of infimal-convolution and Jensen’s inequality, along with (\ref{eqn:transport_tensor}) and (\ref{eqn:entropy_tensor}).</p>

<p>Let’s specialize this to our typical cost functions:</p>

<p><strong>Corollary 2:</strong> (Tensorization of $T_1$) Suppose $\mu$ verifies $T_1(C)$ on $(\mathcal{X},d)$. Then $\mu^{\otimes n}$ verifies the inequality $T_1(nC)$ on $\mathcal{X}^n$ equipped with $d_1^n(x,y):=\sum_{i=1}^n d(x_i,y_i)$. Consequently, for all $1$-Lipschitz functions $f$:</p>

\[\begin{equation*}
\mu^{\otimes n}(f&gt;m_f+r+r_0)\leq e^{-\frac{r^2}{nC}}
\end{equation*}\]

<p>for $r\geq r_0=\sqrt{n C\log(2)}$.</p>

<p><em>Proof:</em> $T_1(C)$ means:</p>

\[W_1^2(\nu,\mu)=\mathcal{T}^2_d(\nu,\mu)\leq CH(\nu\mid\mu),\]

<p>and hence $\frac{1}{n}\mathcal{T}^2_{d_1^n}(\nu,\mu)\leq CH(\nu\mid\mu)$ (recall that $\alpha(x)=\frac{1}{C}x^2$ in this case). Apply Theorem 4 and Theorem 3 to conclude concentration.</p>

<p><strong>Corollary 3:</strong> (Tensorization of $T_2$) Suppose $\mu$ verifies $T_2(C)$ on $(\mathcal{X},d)$. Then $\mu^{\otimes n}$ verifies the inequality $T_2(C)$ on $\mathcal{X}^n$ equipped with $d_2^n(x,y)=\sqrt{\sum_{i=1}^n d^2(x_i,y_i)}$. Consequently, for all $1$-Lipschitz functions $f$:</p>

\[\begin{equation*}
\mu^{\otimes n}(f&gt;m_f+r+r_0)\leq e^{-\frac{r^2}{C}}
\end{equation*}\]

<p>for $r\geq r_0=\sqrt{2\log 2}$.</p>

<p><em>Proof:</em> $T_2(C)$ means:</p>

\[W_2^2(\nu,\mu)=\mathcal{T}_{d_2^1}(\nu,\mu)\leq CH(\nu\mid\mu).\]

<p>In this case, $\alpha=\frac{1}{C}$, so Theorem 4 yields:</p>

\[\mathcal{T}_{\sum_{i=1}^n d^2}(\nu,\mu^{\otimes n})\leq CH(\nu\mid\mu^{\otimes n}).\]

<p>Observe that:</p>

\[\mathcal{T}_{\sum_{i=1}^n d^2}=\mathcal{T}_{(d_2^n)^2}(\nu,\mu^{\otimes n})\]

<p>and hence this is $T_2(C)$ with this metric.</p>

<p>Hence we see that $T_2$ is much stronger than $T_1$, since it tensorizes to give dimension free concentration. Combining Corollary 3 and Corollary 1, we obtain Theorem 2.</p>

<h3 id="extension-to-strongly-log-concave-measures">Extension to strongly log-concave measures</h3>

<p>Suppose $\mu\in \mathcal{P}(\mathcal{X})$ satisfies $\mu(f&gt;m_f+r+r_0)\leq \ell(r)$ for some $r_0$ and decreasing function $\ell(r)$, for all 1-Lipschitz functions $f$ (i.e. $\mu$ has concentration profile $\ell$). Suppose further that $\nu\in \mathcal{P}(\mathcal{X})$ and that there exists a $1$-Lipschitz transport map $T_{\mu\rightarrow \nu}$ pushing $\mu$ onto $\nu$. Then it is clear that $\nu( f &gt; m_f +r +r_0)\leq \ell(r)$ as well. To see this, observe that for any measurable $A\subset \mathcal{X}$ with $\mu(A)\geq \frac{1}{2}$ we have $\nu(T_{\mu\rightarrow \nu}(A))\geq \frac{1}{2}$ by definition of a transport map, and furthermore that:</p>

\[\begin{align*}
1-\ell(r)&amp; \leq \mu(A^r) \\&amp; =\nu (T_{\mu\rightarrow \nu}(A^r)) \\&amp; \leq \nu((T_{\mu\rightarrow \nu}(A))^r),
\end{align*}\]

<p>by $1$-Lipschitzness of $T_{\mu\rightarrow \nu}.$ In other words, we may transport concentration properties along 1-Lipschitz maps. With this in mind, the following theorem is especially useful:</p>

<p><strong>Theorem 5:</strong> (Caffarelli’s Log-Concave Perturbation Theorem) Let $\mu$ have density $d\mu=\exp(-V)$ and $\nu$ have density $d\nu=\exp(-W)$ such that $\nabla^2 V \lesssim \beta_V I$ and $0 \lesssim \alpha_W I \lesssim \nabla^2 W$, where $\alpha_W$ and $\beta_V$ are positive constants. Then the optimal transport map $T_{\mu\rightarrow \nu}$ is $\sqrt{\beta_V/\alpha_W}$-Lipschitz.</p>

<p>(Interestingly, three recent proofs of this using entropy-regularized optimal transport)</p>

<p><strong>Corollary:</strong> (Concentration for Strongly Log-Concave Measures) Let $\nu\in\mathcal{P}_2(\mathbb{R}^d)$ have density $d\nu=\exp(-W)$ with $W:\mathbb{R}^d\rightarrow \mathbb{R}$ a $\kappa$-strongly log-concave for $\kappa&gt;0$. Then $\nu(f&gt;m_f+r+r_0)\leq e^{-\frac{r^2}{\kappa}}$.</p>

<p><em>Proof:</em> Let $\gamma_\kappa$ be a standard $d$-dimensional Gaussian with variance $2\kappa^2$. Then by applying Theorem 4 and scaling appropriately, $\gamma_\kappa$ satisfies $T_2(\kappa)$, and hence has the above concentration profile by Theorem 3. By Theorem 5, the optimal transport map $T_{\mu\rightarrow \nu}$ is $1$-Lipschitz, so we may transport this concentration profile to $\nu$.</p>]]></content><author><name>Brendan Mallery</name></author><category term="Optimal Transport" /><category term="Concentration of Measure" /><category term="Transportation Inequalities" /><summary type="html"><![CDATA[Here are some notes from back when I was trying to really learn about transport inequalities for the first time. Most of the following is distilled from and . Before getting on, I’ll say that since then there have been a number of times where I was working hard on a problem, only to realize that what I was ``really doing’’ the whole time was establishing a transport inequality. I mention this because (in my opinion), transport inequalities seem opaque and technical when you first come across them, made all the more mysterious by the fact that a lot of the most celebrated results in the field are about establishing them.]]></summary></entry></feed>