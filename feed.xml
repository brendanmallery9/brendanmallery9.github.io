<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://brendanmallery9.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://brendanmallery9.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-09T03:44:16+00:00</updated><id>https://brendanmallery9.github.io/feed.xml</id><title type="html">Brendan Mallery</title><subtitle>&apos;PhD candidate studying optimal transport&apos; </subtitle><entry><title type="html">Transport Inequalities</title><link href="https://brendanmallery9.github.io/blog/2024/Transport-inequalities/" rel="alternate" type="text/html" title="Transport Inequalities"/><published>2024-10-07T00:00:00+00:00</published><updated>2024-10-07T00:00:00+00:00</updated><id>https://brendanmallery9.github.io/blog/2024/Transport-inequalities</id><content type="html" xml:base="https://brendanmallery9.github.io/blog/2024/Transport-inequalities/"><![CDATA[<p>Here are some notes from back when I was trying to really learn about transport inequalities for the first time. Most of the following is distilled from <d-cite key="gozlan2010transport"></d-cite> and <d-cite key="gozlan2015transport"></d-cite>. Before going ahead, I’ll say that since then there have been a number of times where I was working hard on something, only to realize that what I ``really doing’’ the whole time was establishing a transport inequality. I mention this because (in my opinion), transport inequalities seem opaque and technical when you first come across them, made all the more mysterious by the fact that a lot of the most <a href="https://cedricvillani.org/sites/dev/files/old_images//2012/08/014.OV-Talagrand.pdf">celebrated results</a> in the field are about establishing them.</p> <p>So if you are like me and are not particularly motivated by a bunch of statements like “LSI==&gt; T2 ==&gt; T1”, rest assured that these things become unexpectedly handy, unexpectedly often.</p> <h2 id="introduction">Introduction</h2> <p><strong>Definition 1:</strong> (Transport Inequality) Let $\mathcal{X}$ be a Polish space equipped with a cost function $c:\mathcal{X}^2\rightarrow \mathbb{R}$ with $c(x,x)=0$ for all $x\in \mathcal{X}$. Consider a function $J(-\mid-):\mathcal{P}(\mathcal{X})^2\rightarrow [0,\infty]$ and $\alpha:[0,\infty)\rightarrow [0,\infty)$ an increasing function so that $\alpha(0)=0.$ Then $\mu\in \mathcal{P}(\mathcal{X})$ satisfies the transport inequality $\alpha(\mathcalT_c)\leq J$ if:</p> \[\alpha(\mathcalT_c(\nu,\mu))\leq J(\nu\mid\mu), \forall \nu\in\mathcal{P}(\mathcal{X})\] <p>We will primarily concerned with the following family of examples:</p> <p><strong>Definition 2:</strong> (Transport-Entropy Inequality) Let $(\mathcal{X},d)$ be a metric space and let $C&gt;0$. We say that $\mu$ satisfies the inequality <strong>$T_p$</strong> $(C)$ if:</p> \[W^2_p(\mu,\nu)\leq CH(\nu|\mu)\] <p>where $W_p$ is the $p$-Wasserstein distance and $H$ is the relative entropy.</p> <p>We will typically take $p=1,2$. When $p=1$, this is Definition 1 with $\alpha(x)=\frac{1}{C}x^2$. When $p=2$, this is Definition 1 with $\alpha(x)=\frac{1}{C}x$. Observe by Jensen’s inequality, <strong>$T_1</strong>$(C)$ is always weaker than <strong>$T_2$</strong>$(C).$ A basic example is the following:</p> <p><strong>Theorem 1:</strong> (Pinsker’s Inequality ) Let $\mu,\nu \in \mathcal{P}(\mathcal{X})$, and let $d_H$ denote the Hamming distance. Then:</p> \[\mathcalT^2_{d_H}(\mu,\nu)=\|\mu-\nu\|^2_{TV}\leq \frac{1}{2}H(\nu|\mu)\] <p>where $|.|_{TV}$ denotes the total-variation distance.</p> <p>A particularly famous one was proven by Talagrand:</p> <p><strong>Theorem 2:</strong> (<strong>$T_2$</strong> for the Standard Gaussian) The standard Gaussian measure $\gamma$ on $\mathbb{R}^d$ satisfies <strong>$T_2$</strong>$(2)$, and this constant is sharp. \end{mytheo}</p> <p>Before proving Theorem 2, we mention some applications:</p> <p><em>Applications of transportation inequalities:</em></p> <ol> <li>If $\mu$ satisfies a transportation inequality, it is concentrated w.r.t. that cost function. <strong>$T_2$</strong> is particularly useful in that it ‘‘tensorizes’’, allowing one to establish \textit{dimension independent} concentration for $\mu^{\otimes n}$.</li> <li>Transportation inequalities can allow one to prove other functional inequalities of interest.</li> <li>Connections to positive curvature lower bounds</li> </ol> <p>We will see an example of <strong>$T_2$</strong> implying tensorization later. First, we will elaborate on the connection to concentration, via a proof attributed to <a href="https://en.wikipedia.org/wiki/Katalin_Marton">Katalin Marton</a>:</p> <p><strong>Theorem 3</strong> (Marton’s argument) Let $\alpha: \mathbb{R}^+\rightarrow \mathbb{R}^+$ be bijective, and suppose that $\mu\in \mathcal{P}(\mathcal{X})$ satisfies $\alpha(\mathcalT_{d^p})\leq H$ for $p\geq 1$. Then for all measurable $A\subset \mathcal{X}$ with $\mu(A)\geq \frac{1}{2}$, the following concentration inequality holds:</p> \[\mu(\{x\in \mathcal{X}|d(x,A)\leq r\}) \geq 1-e^{-\alpha(r-r_0)},\] <p>for all $r&gt;r_0:=\alpha^{-1}(\log(2))$. Equivalently, for all $1$-Lipschitz $f$, the following holds:</p> \[\mu(\{f&gt;m_f+r+r_0\})\leq e^{-\alpha(r)},\;\;\;\;\;\;r\geq 0\] <p>where $m_f$ is the median of $f$. <em>Proof:</em> Take $A\subset \mathcal{X}$ with $\mu(A)\geq \frac{1}{2}$, and let $B=\mathcal{X}\setminus A^r$. Consider $d\mu_A(x):=\frac{1}{\mu(A)}1_A(x) d\mu(x)$ and $d\mu_B 1_B(x)d\mu(x)$. Any coupling between $\mu_A$ and $\mu_B$ has cost lower bounded by $r$, by definition. Via the triangle inequality we have :</p> \[\begin{align*} r &amp; \leq \mathcalT_{d^p}(\mu_A,\mu_B) \\&amp; \leq \mathcalT_{d^p}(\mu_A,\mu)+\mathcalT_{d^p}(\mu_B,\mu) \\ &amp; \leq \alpha^{-1}(H(\mu_A|\mu)) +\alpha^{-1}(H(\mu_B|\mu)). \end{align*}\] <p>We compute:</p> \[\begin{align*} H(\mu_A|\mu) &amp; = \int \log (\frac{d\mu_A}{d\mu})d\mu_A \\ &amp; = \frac{1}{\mu(A)}\int_A \log(\frac{1}{\mu(A)})d\mu \\&amp; = -\log (\mu(A)) \\&amp; \leq \log 2 \end{align*}\] <p>and \(\begin{align*} H(\mu_B|\mu) &amp; = \frac{1}{\mu(B)}\int_B \log(\frac{1}{\mu(B)})d\mu \\ &amp; = -\log(1-\mu(A^r)) \end{align*}\)</p> <p>Applying $\alpha$ and exponentiating both sides, we obtain:</p> <p>\(\begin{equation*} \mu(A^r)\geq 1- e^{-\alpha(r-r_0)} \end{equation*}\) for all $r\geq r_0$.</p> <p>Hence we see <strong>$T_p$</strong>$\rightarrow$ subgaussian concentration for any $p$. Observe that from the above, <strong>there is no apparent difference between $T_2$ and $T_1$, as both imply subgaussian concentration.</strong></p> <h2 id="proof-of-theorem-2">Proof of Theorem 2</h2> <p>In this section we give a proof of Theorem 2. The strategy will be to prove the result in 1-D then to use \textit{tensorization} properties of <strong>$T_2$</strong>.</p> <p><strong>Lemma 1:</strong> Let $V: \mathbb{R}\rightarrow \mathbb{R}_{\geq 0}$, and let $\mu\in \mathcal{P}_2(\mathbb{R})$ be defined by $d\mu(x)=e^{-V(x)}dx$. Then:</p> \[H(\nu|\mu)\geq \int V(T_{\mu\rightarrow \nu}(x))-V(x)-V'(x)[T_{\mu\rightarrow \nu}(x)-x] d\mu(x)\] <p>where $T_{\mu\rightarrow \nu}$ is the optimal transport map from $\mu$ to $\nu$. <em>Proof:</em> By Brenier’s theorem, $T_{\mu\rightarrow \nu}$ is a monotone map. Assume that $f$ is absolutely continuous and hence we have $\nu=f\mu$. We may write</p> \[\nu((-\infty,T_{\mu\rightarrow \nu}(x)])=\mu((-\infty,x])\] <p>for all $x\in \mathbb{R}$. By definition of pushforwards, this means: \(\int_{-\infty}^{T_{\mu\rightarrow \nu}(x)} f(z)e^{-V(z)} dz=\int_{-\infty}^x e^{-V(z)}dz.\)</p> <p>Differentiating, we have: \(f(T_{\mu\rightarrow \nu}(x))e^{-V(T_{\mu\rightarrow \nu}(x))}T'_{\mu\rightarrow \nu}(x)=e^{-V(x)}.\)</p> <p>Taking logarithms, we get: \(\log f(T_{\mu\rightarrow \nu}(x))+\log T'_{\mu\rightarrow \nu}(x) -V(T_{\mu\rightarrow \nu}(x))=-V(x).\)</p> <p>Integrating both sides w.r.t. $\mu$, we get: \(\begin{align*} \int \log f d\nu &amp; = \int (V(T_{\mu\rightarrow \nu}(x))-V(x) - \log T'_{\mu\rightarrow \nu}(x))e^{-V(x)}dx \end{align*}\)</p> <p>By integration by parts, we have: $\int (T_{\mu\rightarrow \nu}(x)-x)V’(x) e^{-V(x)}dx=\int (T’_{\mu\rightarrow \nu}(x)-1)e^{-V(x)}dx.$ Adding and subtracting this to the above equation we get: \(\begin{align*} \int \log f d\nu &amp; = \int (V(T_{\mu\rightarrow \nu}(x))-V(x)-V'(x)[T_{\mu\rightarrow \nu}(x)-x])d\mu(x) +\int T'_{\mu\rightarrow \nu}(x)-1 -\log T'_{\mu\rightarrow \nu}(x) d\mu(x) \\ &amp; \geq \int V(T_{\mu\rightarrow \nu}(x))-V(x)-V'(x)[T_{\mu\rightarrow \nu}(x)-x]d\mu \end{align*}\)</p> <table> <tbody> <tr> <td>since $b-1-\log b\geq 0$ for all $b&gt;0$. The left hand side is $H(\nu</td> <td>\mu)$ and we are done.</td> </tr> </tbody> </table> <p><strong>Corollary 1:</strong> (<strong>$T_1$</strong> for the standard 1D Gaussian) Let $\gamma$ be the standard 1-D Gaussian. Then $\gamma$ satisfies <strong>$T_2$($2$)</strong>: \(H(\nu|\mu)\geq \frac{1}{2} W_2^2(\nu,\gamma)\)</p> <p><em>Proof:</em> Applying Lemma 1\ with $V(x)=x^2/2+\log(2\pi)/2$ we get: \(H(\nu|\gamma)\geq \int \frac{(T_{\gamma\rightarrow \nu}(x)-x)^2}{2} d\gamma(x) = \frac{W_2^2(\nu,\gamma)}{2}.\)</p> <p>We remark that the general case may be proven by generalizing the above argument. Instead, we proceed by establishing the tensorization property for transport inequalities. First we define infimum convolution:</p> <p><strong>Definition:</strong> Let $\alpha_1,\alpha_2 : [0,\infty)\rightarrow [0,\infty)$. Then the infimum convolution of $\alpha_1,\alpha_2$ is defined: \(\alpha_1 \square \alpha_2(t)=\inf \{\alpha_1(t_1)+\alpha_2(t_2):t_1+t_2=t\}.\)</p> <p><strong>Remark:</strong></p> <ul> <li>Infimal-convolution is integral convolution in the min-plus algebra</li> <li>Infimum-convolution preserves convexity.</li> <li>If $\alpha_1,\alpha_2$ are increasing then $\alpha_1\square \alpha_2$ is as well.</li> <li>The proximal operator $\text{prox}^\mu_f(x)=\inf_w f(x)+\frac{1}{2\mu}|x-w|^2$ is an example of an infimal-convolution.</li> <li> <p>Note we may write Kantorovich duality as: \(W_p(\mu,\nu)^p=\displaystyle\sup_{f \in C_b(\mathcal{\mathbb{R}^d})} \int f\square \|.\|^p d\nu - \int f d\mu.\)</p> </li> <li><strong>Important:</strong> For convex, increasing $\alpha$, we have: $\alpha^{\square n}(t)=n \alpha (t/n)$ for all $t\geq 0$. (Proof: Set $n=2$ and take a derivative, then generalize.)</li> </ul> <p><strong>Theorem 4:</strong> Suppose $\mu$ verifies <strong>$T_1$</strong>$(C)$ on $(\mathcal{X},d)$. Then $\mu^{\otimes n}$ verifies the inequality <strong>$T_1$</strong>$(nC)$ on $\mathcal{X}^n$ equipped with $d_1^n(x,y):=\sum_{i=1}^n d(x_i,y_i)$. Consequently, for all $1$-Lipschitz functions $f$: \(n \alpha\left(\frac{\mathcalT_{c^{\oplus n}} (\nu,\mu^{\otimes n})}{n}\right) \leq H(\nu|\mu^{\otimes n}),\)</p> <p>for all $\nu\in \mathcal{P}(\mathcal{X}^n)$, where $c^{\oplus n}(x,y)=\sum_{i=1}^n c(x_i,y_i)$.</p> <p><em>Proof:</em> We will prove this for $n=2$, and the result extends to arbitrary $n$ by induction: Let $\nu\in \mathcal{P}(\mathcal{X}\times \mathcal{X})$. Then we may define the conditional expectation w.r.t. the first coordinate as follows: \(d\nu(x_1,x_2)=d\nu_1(x_1)d\nu_2^{x_1}(x_2).\)</p> <p>With this notation established, we establish some useful facts: \(\begin{equation}\mathcalT_{c_1\otimes c_2}(\nu,\mu_1\otimes \mu_2)\leq \mathcalT_{c_1}(\nu_1,\mu_1)+\int \mathcalT_{c_2}(\nu_2^{x_1},\mu_2)d\nu_1(x_1),\label{eqn:transport_tensor}\end{equation}\)</p> \[\begin{equation}H(\nu|\mu_1\otimes \mu_2)=H(\nu_1|\mu_1)+\int H(\nu_2^{x_1}|\mu_2)d\nu_1(x_1)\label{eqn:entropy_tensor} \end{equation}\] <p>(\ref{eqn:transport_tensor}) is established via the coupling that transports ‘‘optimally on slices’’ of $\nu$ and $\mu_1\otimes \mu_2$. (\ref{eqn:entropy_tensor}) is a famous property known as the disintegration of entropy. We compute:</p> \[\begin{align*} \alpha_1\square \alpha_2 (\mathcalT_{c_1\oplus c_2}(\nu,\mu_1\otimes\mu_2) &amp;\leq \alpha_1\square \alpha_2\left(\mathcalT_{c_1}(\nu_1,\mu_1)+\int \mathcalT_{c_2}(\nu_2^{x_1},\mu_2)d\nu_1(x_1)\right)\\ &amp;\leq \alpha_1(\mathcalT_{c_1}(\nu_1,\mu_1))+\alpha_2\left(\int \mathcalT_{c_2}(\nu_2^{x_1},\mu_2)d\nu_1(x_1)\right) \\ &amp; \leq \alpha_1(\mathcalT_{c_1}(\nu_1,\mu_1))+\int \alpha_2(\mathcalT_{c_2}(\nu_2^{x_1},\mu_2))d\nu_1(x_1)\\&amp; \leq H(\nu_1|\mu_1)+\int H(\nu_2^{x_1}|\mu_2)d\nu_1(x_1) \\&amp; = H(\nu|\mu_1\otimes \mu_2) \end{align*}\] <p>where we used the fact that $\alpha_1\square \alpha_2$ is increasing, the definition of infimal-convolution and Jensen’s inequality, along with (\ref{eqn:transport_tensor}) and (\ref{eqn:entropy_tensor}).</p> <p>Let’s specialize this to our typical cost functions: <strong>Corollary 2:</strong> (Tensorization of <strong>$T_1$</strong>) Suppose $\mu$ verifies <strong>$T_1$</strong>$(C)$ on $(\mathcal{X},d)$. Then $\mu^{\otimes n}$ verifies the inequality <strong>$T_1$</strong>$(nC)$ on $\mathcal{X}^n$ equipped with $d_1^n(x,y):=\sum_{i=1}^n d(x_i,y_i)$. Consequently, for all $1$-Lipschitz functions $f$:</p> \[\begin{equation*} \mu^{\otimes n}(f&gt;m_f+r+r_0)\leq e^{-\frac{r^2}{nC}} \end{equation*}\] <p>for $r\geq r_0=\sqrt{n C\log(2)}$.</p> <table> <tbody> <tr> <td><em>Proof:</em> <strong>$T_1(C)$</strong> means $W_1^2(\nu,\mu)=\mathcalT^2_d(\nu,\mu)\leq CH(\nu</td> <td>\mu)$, and hence $\frac{1}{n}\mathcalT^2_{d_1^n}(\nu,\mu)\leq CH(\nu</td> <td>\mu)$ (recall that $\alpha(x)=\frac{1}{C}x^2$ in this case). Apply Theorem 4 and Theorem 3 to conclude concentration.</td> </tr> </tbody> </table> <p><strong>Corollary 3:</strong> (Tensorization of <strong>$T_2$</strong>) Suppose $\mu$ verifies <strong>$T_2$</strong>$(C)$ on $(\mathcal{X},d)$. Then $\mu^{\otimes n}$ verifies the inequality <strong>$T_2$</strong>$(C)$ on $\mathcal{X}^n$ equipped with $d_2^n(x,y)=\sqrt{\sum_{i=1}^n d^2(x_i,y_i)}$. Consequently, for all $1$-Lipschitz functions $f$:</p> \[\begin{equation*} \mu^{\otimes n}(f&gt;m_f+r+r_0)\leq e^{-\frac{r^2}{C}} \end{equation*}\] <p>for $r\geq r_0=\sqrt{2\log 2}$. <em>Proof:</em> <strong>$T_2$</strong>$(C)$ means $W^2<em>2(\nu,\mu)=\mathcalT</em>{d_2^1}(\nu,\mu)\leq CH(\nu|\mu)$. In this case, $\alpha=\frac{1}{C}$, so Theorem \ref{thm:tensorization} yields $\mathcalT_{\sum_{i=1}^n d^2}(\nu,\mu^{\otimes n})\leq CH(\nu|\mu^{\otimes n})$. Observe that $\mathcalT_{\sum_{i=1}^n d^2}=\mathcalT_{(d_2^n)^2}(\nu,\mu^{\otimes n})$, and hence this is <strong>$T_2$</strong>$(C)$ with this metric.</p> <p>Hence we see that <strong>$T_2$</strong> is much stronger than <strong>$T_1$</strong>, since it tensorizes to give dimension free concentration. Combining Corollary 3 and Corollary 1, we obtain Theorem 2.</p> <h3 id="extension-to-strongly-log-concave-measures">Extension to strongly log-concave measures</h3> <p>Suppose $\mu\in \mathcal{P}(\mathcal{X})$ satisfies $\mu(f&gt;m_f+r+r_0)\leq \ell(r)$ for some $r_0$ and decreasing function $\ell(r)$, for all 1-Lipschitz functions $f$ (i.e. $\mu$ has concentration profile $\ell$). Suppose further that $\nu\in \mathcal{P}(\mathcal{X})$ and that there exists a $1$-Lipschitz transport map $T_{\mu\rightarrow \nu}$ pushing $\mu$ onto $\nu$. Then it is clear that $\nu( f &gt; m_f +r +r_0)\leq \ell(r)$ as well. To see this, observe that for any measurable $A\subset \mathcal{X}$ with $\mu(A)\geq \frac{1}{2}$ we have $\nu(T_{\mu\rightarrow \nu}(A))\geq \frac{1}{2}$ by definition of a transport map, and furthermore that:</p> \[\begin{align*} 1-\ell(r)&amp; \leq \mu(A^r) \\&amp; =\nu (T_{\mu\rightarrow \nu}(A^r)) \\&amp; \leq \nu((T_{\mu\rightarrow \nu}(A))^r), \end{align*}\] <p>by $1$-Lipschitzness of $T_{\mu\rightarrow \nu}.$ In other words, we may transport concentration properties along 1-Lipschitz maps. With this in mind, the following theorem is especially useful:</p> <p><strong>Theorem 5:</strong> (Caffarelli’s Log-Concave Perturbation Theorem) Let $\mu$ have density $d\mu=\exp(-V)$ and $\nu$ have density $d\nu=\exp(-W)$ such that $\nabla^2 V \lesssim \beta_V I$ and $0 \lesssim \alpha_W I \lesssim \nabla^2 W$, where $\alpha_W$ and $\beta_V$ are positive constants. Then the optimal transport map $T_{\mu\rightarrow \nu}$ is $\sqrt{\beta_V/\alpha_W}$-Lipschitz.</p> <p>(Interestingly, three recent proofs of this using entropy-regularized optimal transport)</p> <p><strong>Corollary:</strong> (Concentration for Strongly Log-Concave Measures) Let $\nu\in\mathcal{P}_2(\mathbb{R}^d)$ have density $d\nu=\exp(-W)$ with $W:\mathbb{R}^d\rightarrow \mathbb{R}$ a $\kappa$-strongly log-concave for $\kappa&gt;0$. Then $\nu(f&gt;m_f+r+r_0)\leq e^{-\frac{r^2}{\kappa}}$.</p> <p><em>Proof:</em> Let $\gamma_\kappa$ be a standard $d$-dimensional Gaussian with variance $2\kappa^2$. Then by applying Theorem \ref{thm:tensorization} and scaling appropriately, $\gamma_\kappa$ satisfies <strong>$T_2$</strong>$(\kappa)$, and hence has the above concentration profile by Theorem 3. By Theorem 5, the optimal transport map $T_{\mu\rightarrow \nu}$ is $1$-Lipschitz, so we may transport this concentration profile to $\nu$.</p>]]></content><author><name>Brendan Mallery</name></author><category term="distill"/><category term="formatting"/><summary type="html"><![CDATA[an example of a distill-style blog post and main elements]]></summary></entry></feed>